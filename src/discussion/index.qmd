---
number-offset: 4
---

# Discussion

Notre capacité à tirer profit des nouvelles sources de données et des méthodes de *data science* innovantes dépend largement de notre aptitude à les intégrer dans des chaînes de production efficaces et robustes. Le cas d'usage présenté dans cet article — classifier les domaines d'activité des entreprises à l'aide de méthodes d'apprentissage automatique — illustre bien l'intrication des dimensions méthodologique et informatique dans le paradigme de la *data science*. Le passage rapide entre les phases d'expérimentation et de mise en production du projet a été largement favorisé par les possibilités de développement itératif et d'automatisation permis par les technologies *cloud*. Le projet s'est en effet construit par briques successives, d'une API minimale à un projet complet intégrant une couche de supervision permettant d'améliorer le modèle déployé au fil du temps. L'appropriation des technologies *cloud* par les membres des équipes impliquées a également été un facteur déterminant. Les environnements mis à disposition via le catalogue du SSP Cloud ont permis à la fois de développer rapidement des preuves de concept pour prouver l'intérêt de l'ajout de nouveaux composants et de favoriser la reproductibilité des expérimentations et leur passage en production en mettant en application les principes du MLOps.

Bien entendu, le choix des technologies *cloud* n'est pas un choix neutre pour l'organisation. Ces technologies peuvent représenter des coûts d'entrée important dans la mesure où elles nécessitent un investissement dans le développement et le maintien de compétences qui ne sont pas toujours présentes dans les INS. Sur ce point, le fait que de plus en plus d'organisations actives dans l'écosystème de la donnée tendent à s'orienter vers des solutions *cloud* rassure néanmoins sur la pertinence de ce choix stratégique. De même, la transition vers les technologies *cloud* impose des coûts d'entrée pour les statisticiens. Tout d'abord, ils peuvent être confrontés à une perte de repères quant à l'endroit où les calculs sont réellement effectués. Bien qu'ils soient déjà habitués à effectuer des calculs sur des serveurs centralisés plutôt que sur un poste de travail, le conteneur ajoute une couche d'abstraction qui rend cet emplacement difficile à appréhender au départ. Mais le changement le plus perturbant dans ce paradigme est la perte de persistance des données. Dans les configurations traditionnelles — qu'il s'agisse d'un ordinateur personnel ou d'un serveur accessible via un bureau virtuel — le code, les données et l'environnement de calcul sont souvent mélangés dans une sorte de boîte noire. À l'inverse, les conteneurs, par construction, n'ont pas de persistance. Si le stockage objet fournit cette persistance, une utilisation adéquate de ces infrastructures exige une variété d'outils et de compétences : utilisation d'un système de contrôle de version pour le code (e.g. Git), interaction avec l'API de stockage pour enregistrer les données, gestion de fichiers de configuration et/ou de secrets et variables d'environnement, etc. En un sens, ces coûts d'entrée peuvent être considérés comme le "prix" de l'autonomie : grâce aux technologies *cloud*, les statisticiens ont désormais accès à des environnements flexibles leur permettant d’expérimenter plus librement, mais cette autonomie exige une montée en compétences significative, qui peut être intimidante et, *in fine*, limiter cette adoption. Cependant, l'expérience montre que cet effet peut être largement atténué grâce à un effort join de formation des statisticiens aux bonnes pratiques de développement et d’accompagnement des projets lors de leur transition vers des infrastructures *cloud*[^formations].

[^formations]: Des ressources comme la formation aux bonnes pratiques de développement avec Git et R ([https://github.com/InseeFrLab/formation-bonnes-pratiques-git-R](https://github.com/InseeFrLab/formation-bonnes-pratiques-git-R)) ou encore le cours de mise en production des projets de *data science* à l'ENSAE ([https://ensae-reproductibilite.github.io/website/](https://ensae-reproductibilite.github.io/website/)) visent à combler ce besoin. De même, les équipes Innovation, Datascience et KubeApp proposent des accompagnements des équipes projets pour faciliter l'adoption des nouveaux environnements de *self* et de déploiement. Ces différentes initiatives participent de cet effort de montée en compétence collective sur les outils informatiques qui favorisent la qualité des projets et ainsi leur passage en production.

Bien que des solutions comme Onyxia permettent de démocratiser l'accès aux technologies *cloud* pour les statisticiens, il s'agit de solutions de nature essentiellement technique. L'intégration effective des méthodes de *data science* dans la production statistique des INS soulève des défis plus larges, d'ordre organisationnel. Une leçon majeure à tirer du déploiement du premier modèle de codification de l'APE est la nécessité de surmonter la segmentation des compétences entre les équipes informatiques, métier et innovation. Par nature, les projets de *machine learning* impliquent un large éventail de compétences — connaissance du domaine métier, entraînement et amélioration des modèles, déploiement et supervision — et nécessitent donc une collaboration étroite entre des agents qui possèdent des connaissances et utilisent des langages de programmation variés. Si la continuité apportée par les technologies *cloud* favorise manifestement la collaboration entre les différentes équipes d'un projet, répondre pleinement à ces défis nécessite des choix qui dépassent le domaine technique. Par exemple, intégrer certaines compétence en *data science* directement au sein des équipes métier, en complément des équipes d'innovation centralisées, permettrait de favoriser une meilleure communication des besoins. De même, recruter des profils qui ne sont pas traditionnellement présents dans les INS, comme les *data engineers* ou les *ML engineers*, pourrait apporter de nouvelles compétences clés se situant à l'intersection des méthodologies statistiques et des techniques informatiques. Au final, la transition vers une production statistique intégrant plus fortement les méthodes de *data science* implique une stratégie équilibrée qui lie des solutions techniques favorisant l'autonomie et l'adoption de nouveaux outils avec des ajustements organisationnels et humains, dans une culture de collaboration, de formation continue et d’innovation.
