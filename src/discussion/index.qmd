---
number-offset: 4
---

# Discussion

Notre capacité à tirer profit des nouvelles sources de données et des méthodes innovantes dépend largement de notre aptitude à les intégrer dans des chaînes de production efficaces et robustes. Le cas d'usage présenté dans cet article — classifier les domaines d'activité des entreprises à l'aide de méthodes d'apprentissage automatique — illustre bien l'intrication des dimensions méthodologique et informatique dans le paradigme de la *data science*. Le passage rapide entre les phases d'expérimentation et de mise en production du projet a été largement favorisé par les possibilités de développement itératif et d'automatisation permises par les technologies *cloud*. Le projet s'est en effet construit par briques successives, d'une API minimale à un projet complet intégrant une couche de supervision permettant d'améliorer le modèle déployé au fil du temps. L'appropriation des technologies *cloud* par les membres des équipes impliquées a également été un facteur déterminant. Les environnements mis à disposition via le catalogue du SSP Cloud ont permis à la fois de développer rapidement des preuves de concept pour prouver l'intérêt de l'ajout de nouveaux composants et de favoriser la reproductibilité des expérimentations et leur passage en production en mettant en application les principes du MLOps.

Bien entendu, le choix des technologies *cloud* n'est pas un choix neutre pour l'organisation. Ces technologies peuvent représenter des coûts d'entrée important dans la mesure où elles nécessitent un investissement dans le développement et le maintien de compétences qui ne sont pas toujours présentes dans les INS. Sur ce point, le fait que de plus en plus d'organisations actives dans l'écosystème de la donnée tendent à s'orienter vers des solutions *cloud* rassure néanmoins sur la pertinence de ce choix stratégique. De même, la transition vers les technologies *cloud* impose des coûts d'entrée pour les statisticiens. Tout d'abord, ils peuvent être confrontés à une perte de repères quant à l'endroit où les calculs sont réellement effectués. Bien qu'ils soient déjà habitués à effectuer des calculs sur des serveurs centralisés plutôt que sur un poste de travail, le conteneur ajoute une couche d'abstraction qui rend cet emplacement difficile à appréhender au départ. Mais le changement le plus perturbant dans ce paradigme est sans doute la perte de persistance des données. Dans les configurations habituelles — qu'il s'agisse d'un ordinateur personnel ou d'un serveur accessible via un bureau virtuel — le code et les données sont généralement mélangés dans un dossier de projet sur le système de fichiers, auquel l'environnement de calcul accède de manière persistante. À l'inverse, les conteneurs n'ont pas de persistance : à partir de données en entrée, ils réalisent une ou plusieurs actions qui produisent éventuellement d'autres données en sortie, puis sont amenés à s'éteindre. Si le stockage objet fournit une solution pour la persistance de ces données en entrée et en sortie, l'absence de persistance dans le temps de l'environnement de calcul lui-même implique un changement des pratiques : utilisation d'un système de contrôle de version pour le code (e.g. Git), interaction avec le stockage distant pour importer et enregistrer les données, gestion de fichiers de configuration et/ou de secrets qui paramétrisent l'environnement, etc. En un sens, ces coûts d'entrée peuvent être considérés comme le "prix" de l'autonomie : grâce aux technologies *cloud*, les statisticiens ont désormais accès à des environnements flexibles leur permettant d’expérimenter plus librement, mais cette autonomie exige une montée en compétences significative qui peut, *in fine*, limiter leur adoption. Notre expérience montre néanmoins que cette limite peut être largement atténuée grâce à un effort soutenu de formation aux bonnes pratiques de développement et d’accompagnement des projets lors de leur transition vers des infrastructures *cloud*[^formations].

[^formations]: Des ressources comme la formation aux bonnes pratiques de développement avec Git et R ([https://github.com/InseeFrLab/formation-bonnes-pratiques-git-R](https://github.com/InseeFrLab/formation-bonnes-pratiques-git-R)) ou encore le cours de mise en production des projets de *data science* à l'ENSAE ([https://ensae-reproductibilite.github.io/website/](https://ensae-reproductibilite.github.io/website/)) visent à combler ce besoin. De même, les équipes Innovation, Datascience et KubeApp proposent des accompagnements des équipes projets pour faciliter l'adoption des nouveaux environnements de *self* et de déploiement. Ces différentes initiatives participent de cet effort de montée en compétence collective sur les outils informatiques qui favorisent la qualité des projets et ainsi leur passage en production.

Bien que des solutions comme Onyxia permettent de démocratiser l'accès aux technologies *cloud*, il s'agit de solutions de nature essentiellement technique. L'intégration effective des méthodes de *data science* dans la production statistique des INS soulève des défis plus larges, d'ordre organisationnel. Une leçon majeure à tirer du déploiement du premier modèle de codification de l'APE est la nécessité de surmonter la segmentation des compétences entre les équipes informatiques, métier et innovation. Par nature, les projets de *machine learning* impliquent un large éventail de compétences — connaissance du domaine métier, entraînement et amélioration des modèles, déploiement et supervision — et nécessitent donc une collaboration étroite entre des agents dont les métiers et les pratiques, notamment en matière de langage de programmation, diffèrent significativement. Si la continuité apportée par les technologies *cloud* favorise la collaboration entre les différentes équipes d'un projet, répondre pleinement à ces défis implique des choix qui dépassent le domaine technique. Par exemple, intégrer certaines compétence en *data science* directement au sein des équipes métier, en complément des équipes d'innovation centralisées, permettrait à de tels profils de jouer un rôle d'interface capable à la fois de modéliser les problématiques métier et de savoir les restituer dans un langage commun aux équipes techniques chargées de leur implémentation et/ou de leur déploiement. De même, le recrutement de profils qui ne sont pas traditionnellement présents dans les INS comme les *data engineers* — profil hybride qui allie compétences en développement et connaissance fine des architectures de traitement de la donnée — pourrait apporter de nouvelles compétences clés pour favoriser la mise en production des projets innovants, comme le recommande le rapport sur "L’Évaluation des besoins de l'État en compétences et expertises en matière de données" [@dinuminsee2021]. Au final, la transition vers une production statistique intégrant plus fortement les méthodes de *data science* implique une stratégie mixte faisant intervenir à la fois des solutions techniques comme les technologies *cloud* qui favorisent l'autonomie, des ajustements humains qui diversifient les compétences de traitement de la donnée, et des évolutions organisationnelles qui favorisent une culture de collaboration, de formation continue et d’innovation.
