---
number-offset: 3
---

# Faire du *machine learning* en production : l'exemple de la codification de l'APE {#sec-mlops-fr}

Ce chapitre vise à illustrer comment l’Insee a réussi à déployer son premier modèle de machine learning (ML) en production. Il propose une description détaillée de l’approche MLOps à laquelle ce projet s’est efforcé d’adhérer, en mettant l’accent sur les différentes technologies employées. En particulier, nous soulignons le rôle crucial des technologies *cloud* qui ont permis la construction du projet de manière itérative, ainsi que la manière dont Onyxia a grandement facilité cette construction en fournissant des environnements de développement flexibles et des outils pour entraîner, déployer et surveiller les modèles des modèles d'apprentissage automatique. De plus, la convergence entamée des environnement de *self* ($LS^3$), de développement (KubeDev) et de production (KubeProd) constitue une réelle avancée pour faciliter la mise en production d'autres modèles d'apprentissage automatique[^pcs2020]. Le projet présenté est disponible en open source[^codif-team] et reste en cours de développement actif.

[^pcs2020]: Un modèle similaire pour améliorer la codification de la PCS dans la nomenclature PCS2020 a également été déployé en production.
[^codif-team]: [https://github.com/orgs/InseeFrLab/teams/codification-ape/repositories]()


## Améliorer la codification de l'APE à l'aide de méthodes d'apprentissage automatique

### Motivation

Les tâches de codification sont des opérations bien connues des instituts statistiques, et peuvent parfois être complexes en raison de la taille des nomenclatures. À l’Insee, un outil sophistiqué appelé Sicore a été développé dans les années 1990 pour effectuer diverses tâches de classification [@meyer_sicore_1997]. Cet outil repose sur un ensemble de règles déterministes permettant d'identifier les codes corrects à partir d'un libellé textuel en se basant sur un fichier de référence comprenant un certain nombre d'exemples. Chaque libellé d'entrée est soumis à ces règles et, lorsqu'un code correct est reconnu, il est attribué au libellé. En revanche, si le libellé n'est pas reconnu, il est soumis à une procédure de reprise manuelle par des gestionnaires de l'Insee.

Deux raisons principales ont motivé l’expérimentation de nouvelles méthodes de codification. Premièrement, un changement interne est survenu avec la refonte du répertoire statistique des entreprises en France (Sirene), qui liste toutes les entreprises et leur attribue un identifiant unique utilisé par les administrations publiques, le numéro Siren. Les principaux objectifs de cette refonte étaient d’améliorer la gestion quotidienne du répertoire pour les agents de l’Insee et de réduire les délais d’attente pour les entreprises. Par ailleurs, au niveau national, le gouvernement a lancé dans le cadre de la loi PACTE (n° 2019-486 du 22 mai 2019) un guichet unique pour les formalités des entreprises, offrant aux chefs d'entreprises plus de flexibilité dans la description de leurs activités principales mais les rendant ainsi mécaniquement plus verbeux que précédemment. Les tests initiaux ont révélé que Sicore n’était plus adapté pour effectuer la codification APE, puisque seulement $30\%$ des liasses d'entreprises étaient automatiquement codées, et donc $70\%$ partaient en reprise manuelle. Les équipes en charge du répertoire Sirene, déjà confrontées à des charges de travail importantes et à de fortes contraintes opérationnelles, ne pouvaient pas voir leur charge augmentée par une re-codification manuelle du fait du caractère très chronophage de la tâche. Ainsi, en mai 2022, la décision a été prise d’expérimenter de nouvelles méthodes pour effectuer cette tâche de codification, avec pour objectif de les utiliser en production dès le 1er janvier 2023, date de lancement du nouveau répertoire Sirene.

Trois parties prenantes étaient donc impliquées dans ce projet : l’équipe métier (division RIAS[^rias]), responsable de la gestion du répertoire statistique des entreprises ; l’équipe informatique, en charge du développement des applications liés au fonctionnement du répertoire ; et l’équipe d’innovation (l'unité SSP Lab), responsable de la mise en œuvre du nouvel outil de codification.

[^rias]: Répertoire Interadministratif Sirene

### La tâche de codification

Le projet consiste en un problème classique de classification dans le cadre du traitement du langage naturel (NLP). À partir d'une description textuelle de l'activité d'une entreprise, l'objectif est de prédire la classe associée dans la nomenclature APE. Cette classification présente la particularité d'être hiérarchique et comporte cinq niveaux différents[^niveaux] : section, division, groupe, catégorie et sous-catégorie. Au total, la nomenclature comprend 732 sous-classes, ce qui correspond au niveau le plus fin de la nomenclature et pour lequel on souhaite réaliser la codification. La table @tbl-nace-nomenclature fournit un exemple de cette structure hiérarchique.

[^niveaux]: En réalité, il existe cinq niveaux en France, mais seulement quatre au niveau européen.

::: {#tbl-nace-nomenclature}

| **Niveau** | **NAF** | **Libellé** | **Taille** |
|-------|------|----------------------|-|
| Section     | H     |Transports et entreposage|21|
| Division    | 52    |Entreposage et services auxiliaires des transports|88|
| Groupe      | 522   |Services auxiliaires des transports|272|
| Catégorie      | 5224  |Manutention|615|
| **Sous-catégorie** | [**5224A**]{.red2} |**Manutention portuaire**|[**732**]{.orange}|

Exemples d'éléments à différents niveaux de la nomenclature APE

:::

Avec la mise en place du guichet unique, les chefs d'entreprise décrivent désormais leur activité dans un champ de texte libre. Par conséquent, les nouveaux libellés diffèrent fortement des libellés harmonisés précédemment reçus. Il a donc été décidé de travailler avec des modèles d'apprentissage automatique, reconnus pour leur efficacité sur les tâches de classification supervisée de texte [@li2022survey]. Cela représente un changement de paradigme significatif pour l’Insee, puisque le *machine learning* n'est pas traditionnellement utilisé dans la production des statistiques publiques. De plus, la perspective de mettre le nouveau modèle en production a été envisagée dès le début du projet, orientant de nombreux choix méthodologiques et techniques. Ainsi, plusieurs décisions stratégiques ont dû être rapidement prises, notamment en ce qui concerne la méthodologie, le choix d’un environnement de développement cohérent avec l’environnement de production cible, et l’adoption de méthodes de travail collaboratif.

### Méthodologie

La classification textuelle à partir des champs de texte libre fournis par les chefs d’entreprise est une tâche complexe : les descriptions d’activité sont relativement courtes et contiennent donc peu d’information statistique, peuvent inclure des fautes d’orthographe et nécessitent souvent une expertise métier pour être correctement codées. Pour une telle tâche, les méthodes traditionnelles d’analyse textuelle, comme la vectorisation par comptage ou TF-IDF, sont souvent insuffisantes, tandis que les méthodes d’intégration basées sur des réseaux de neurones tendent à donner de meilleurs résultats [@li2022survey]. Cependant, ces architectures nécessitent souvent des ressources de calcul importantes, et peuvent exiger du matériel spécifique, comme des GPUs, afin obtenir une latence acceptable lors de l’inférence. Ces contraintes ont, dans un premier temps, éloigné des modèles plus performants, tels que les modèles Transformer, et ont orientés le choix vers le modèle fastText [@joulin2016bag], un réseau de neurone plus simple basé sur des plongements lexicaux. Le modèle fastText est extrêmement rapide à entraîner, et l’inférence ne nécessite pas de GPU pour obtenir un temps de latence faible. En outre, le modèle a donné d'excellents résultats pour le cas d'usage présenté, qui, compte tenu des contraintes de temps et de ressources humaines, étaient largement suffisants pour améliorer le processus existant. Enfin, l’architecture du modèle est relativement simple, ce qui facilite la communication et l’adoption au sein des différentes équipes de l’Insee.

Le modèle fastText repose sur une approche de sac de mots (*bag-of-words*) pour obtenir des plongements lexicaux (*word embeddings*) et une couche de classification basée sur la régression logistique[^embeddings]. L’approche sac de mots consiste à représenter un texte comme un ensemble de représentations vectorielles de chacun des mots qui le composent. La spécificité du modèle fastText, par rapport à d'autres approches basées sur des principes similaires, est que les plongements lexicaux ne sont pas seulement calculés sur les mots, mais aussi sur des *n-grams* de mots et de caractères, fournissant ainsi plus de contexte et réduisant les biais liés aux fautes d’orthographe. Le plongement lexical d’une phrase est calculé comme une fonction des plongements lexicaux des mots (et n-grams de mots et de caractères), généralement une moyenne. Dans le cas de la classification textuelle supervisée, la matrice de plongement et les poids du classifieur sont appris simultanément lors de l’entraînement par descente de gradient, en minimisant la fonction de perte d’entropie croisée. La @fig-fasttext présente la *pipeline* complete des opérations effectuées par fastText sur un exemple de texte en entrée.

[^embeddings]: Pour une introduction (en Anglais) à la fois visuelle et intuitive aux plongements lexicaux, voir : [https://jalammar.github.io/illustrated-word2vec/]()  

![Aperçu simplifié du processus derrière la classification textuelle avec fastText](/figures/fasttext.png){#fig-fasttext width="125%"}

## Une approche orientée production et MLOps

Dès le début du projet, l’objectif était d’aller au-delà de la simple expérimentation et d'aboutir à une mise en production rapide du modèle. Par ailleurs, ce projet pilote avait également pour but de servir de référence pour les futurs projets de *machine learning* à l’Insee. L'équipe du projet a donc cherché à appliquer les bonnes pratiques de développement dès les premières étapes du projet : respect des standards communautaires de qualité du code, utilisation de scripts pour le développement au lieu de notebooks, construction d’une structure modulaire semblable à un *package*, etc. Cependant, par rapport aux projets de développement traditionnels, les projets de *machine learning* présentent des caractéristiques spécifiques qui nécessitent l’application d’un ensemble de bonnes pratiques complémentaire, regroupées sous le nom de MLOps.

### Du DevOps au MLOps {#sec-devops-mlops}

Le DevOps est un ensemble de pratiques conçu pour favoriser la collaboration entre les équipes de développement (*Dev*) et d'opérations (*Ops*). L’idée fondamentale est d’intégrer tout le cycle de vie d’un projet dans un continuum automatisé. Un outil important pour atteindre cette continuité sont les *pipeline*s CI/CD. Avec l'intégration continue (*Continuous Integration* ou CI), chaque *commit* de sur le code source du projet déclenche un processus d'opérations standardisées, telles que la construction de l’application, son test et la mise à disposition d'une version de l'application. Ensuite, le déploiement continu (*Continuous Deployment* ou CD) consiste en des outils pour automatiser le déploiement du nouveau code et limiter les interventions manuelles, tout en garantissant une supervision appropriée pour assurer la stabilité et la sécurité des processus. Cette approche favorise un déploiement plus rapide et continu des modifications ou ajouts nécessaires de fonctionnalités. En outre, en encourageant la collaboration entre les équipes, le DevOps accélère également le cycle de développement, permettant aux équipes de résoudre les problèmes au fur et à mesure qu'ils surviennent et d'intégrer efficacement les retours tout au long du cycle de vie du projet.

L’approche MLOps peut être vue comme une extension du DevOps, développée pour relever les défis spécifiques liés à la gestion du cycle de vie des modèles de ML. Fondamentalement, DevOps et MLOps partagent le même objectif : construire des logiciels de manière plus automatisée et robuste. La principale différence réside dans le fait qu'avec le MLOps, le logiciel inclut également une composante de *machine learning* (ML). Par conséquent, le cycle de vie du projet devient plus complexe. Le modèle de ML sous-jacent doit être réentraîné régulièrement afin d’éviter toute perte de performance au fil du temps. L’ingestion des données doit également être intégrée au processus, car de nouvelles données peuvent être utilisées pour améliorer les performances. La @fig-mlops-cycle présente les étapes d’un projet de ML en utilisant une représentation continue, comme cela se fait traditionnellement en DevOps. Cela illustre un principe fondamental du MLOps : la nécessité d’une amélioration continue, décrite plus en détail dans @sec-principles-mlops.

![L'approche MLOps favorise une gestion plus continue du cycle de vie des projets de ML](/figures/mlops-cycle.png){#fig-mlops-cycle}

### Principes du MLOps {#sec-principles-mlops}

Le MLOps repose sur quelques principes fondamentaux qui sont essentiels pour construire des applications de *machine learning* évolutives et adaptées au passage en production.

Le principe essentiel du MLOps est l'amélioration continue, reflétant la nature itérative des projets de ML. Lors de la phase d'expérimentation, le modèle est développé à partir d’un ensemble de données d’entraînement, qui diffère généralement des données de production. Une fois le modèle déployé en production, les nouvelles données sur lesquelles le modèle doit effectuer des prédictions peuvent révéler des informations sur ses performances et ses éventuelles lacunes. Ces informations nécessitent un retour à la phase d’expérimentation, où les *data scientists* ajustent ou redéfinissent leurs modèles pour corriger les problèmes découverts ou améliorer la précision. Ce principe souligne donc l'importance de construire une boucle de rétroaction permettant des améliorations continues tout au long du cycle de vie d'un modèle. L'automatisation, en particulier grâce à l'utilisation de *pipeline*s CI/CD, joue un rôle crucial en rendant la transition entre les phases d’expérimentation et de production plus fluide. La supervision (*monitoring*) est également une composante essentielle de ce processus : un modèle déployé en production doit être continuellement analysé pour détecter d’éventuelles dérives importantes susceptibles de réduire ses performances prédictives et nécessitant des ajustements supplémentaires, comme un ré-entraînement.

Un autre objectif majeur du MLOps est de promouvoir la reproductibilité, en garantissant que toute expérience de ML puisse être reproduite de manière fiable avec les mêmes résultats. Les outils de MLOps facilitent ainsi une sauvegarde détaillée des expériences de ML, incluant les étapes de prétraitement des données, les hyperparamètres des modèles utilisés et les algorithmes d’entraînement. Les données, modèles et codes sont versionnés, permettant aux équipes de revenir à des versions antérieures si une mise à jour ne donne pas les résultats escomptés. Enfin, ces outils aident à produire des spécifications détaillées de l’environnement informatique utilisé pour produire ces expériences — comme les versions des bibliothèques — et reposent souvent sur des conteneurs pour reproduire les mêmes conditions que celles dans lesquelles le modèle initial a été développé.

Enfin, le MLOps vise à favoriser le travail collaboratif. Les projets basés sur le ML impliquent généralement une gamme plus large de profils : équipes métier et équipes de *data science* d’un côté, développeurs et équipes de production informatique de l’autre. Comme le DevOps, le MLOps met donc l’accent sur la nécessité d’une culture collaborative et d'éviter le travail en silos. Les outils de MLOps incluent généralement des fonctionnalités collaboratives, telles que des stockages centralisés pour les modèles de ML ou les caractéristiques (*features*) de ML, qui facilitent le partage des composants entre les membres des équipes et limitent la redondance des développements.


### Implémentation avec MLflow

De nombreux outils ont été développés pour mettre en œuvre l’approche MLOps dans des cas d'usage concrets. Tous visent à appliquer, sous une forme ou une autre, les principes fondamentaux décrits précédemment. Dans ce projet, le choix a été fait de s'appuyer sur un outil open-source populaire nommé MLflow[^mlflow]. Ce choix ne reflète pas une supériorité inhérente de MLflow par rapport à d'autres outil, mais s'explique par un ensemble de bonnes propriétés associées à MLflow, qui en font une solution particulièrement pertinente pour le cas d'usage. Tout d'abord, il couvre l'intégralité du cycle de vie des projets de ML, tandis que d'autres outils peuvent être plus spécialisés sur certaines parties seulement. Ensuite, il offre une grande interopérabilité grâce à une bonne interface avec les bibliothèques populaires de ML — telles que PyTorch, Scikit-learn, XGBoost, etc. — et prend en charge plusieurs langages de programmation — notamment Python, R et Java, couvrant ainsi le spectre des langages couramment utilisés à l'Insee. Enfin, MLflow s’est révélé généralement très accessible, encourageant ainsi son adoption par les membres du projet et facilitant la collaboration continue entre eux.

[^mlflow]: [https://github.com/MLflow/MLflow]()

MLflow fournit un ensemble d'outils intégrés permettant d'implémenter les principes du MLOps efficacement au sein des projets de ML. Les *data scientists* peuvent encapsuler leur travail dans des objects *Projects* qui regroupent le code ML et ses dépendances, garantissant que chaque expérimentation soit reproductible et puisse être ré-exécutée de manière identique. Un projet s'appuie sur un objet *Model*, un format standardisé compatible avec la plupart des bibliothèques de ML et offrant une méthode normalisée pour déployer le modèle, par exemple via une API. Cette interopérabilité et cette standardisation sont essentielles pour soutenir l'amélioration continue du projet, puisque les modèles entraînés avec une multitude de packages peuvent être facilement comparés ou remplacés les uns par les autres sans casser le code existant. Le *Tracking Server* enregistre des informations détaillées sur chaque expérimentation — hyperparamètres, métriques, artefacts et données — ce qui favorise d'une part la reproductibilité et facilite la phase de sélection des modèles grâce à une interface utilisateur permettant de comparer simplement les performances. Une fois la phase d’expérimentation terminée, les modèles sélectionnés sont rajoutés dans le *Model Registry*, où ils sont versionnés et prêts pour le déploiement. Cet entrepôt sert de "magasin" centralisé pour les modèles, permettant aux différents membres ou équipes du projet de gérer collaborativement le cycle de vie du projet. La @fig-mlflow-components illustre les composants principaux de MLflow et la manière dont ils facilitent un flux de travail plus continu et collaboratif au sein d’un projet de ML.

![Composants principaux de MLflow. Source : Databricks.](/figures/mlflow-model-registry.png){#fig-mlflow-components}

## Faciliter le développement itératif avec les technologies cloud

Bien que l’amélioration continue soit un principe fondamental du MLOps, il s'agit également d'un principe exigeant. En particulier, celui-ci nécessite de concevoir et de construire un projet sous la forme d’un *pipeline* intégré, dont les différentes étapes sont principalement automatisées, de l’ingestion des données jusqu’à la supervision du modèle en production. Dans ce contexte, le développement itératif est essentiel pour construire un produit minimum viable qui sera ensuite affiné et amélioré au fil du temps. Cette section illustre comment les technologies *cloud*, via le projet Onyxia, ont été déterminantes pour construire le projet sous forme de composants modulaires interconnectés, renforçant ainsi considérablement la capacité d'amélioration continue.

### Un environnement de développement flexible

Dans un projet de ML, la flexibilité de l’environnement de développement est essentielle. Premièrement, en raison de la diversité des tâches à accomplir : collecte des données, prétraitement, modélisation, évaluation, inférence, supervision, etc. Deuxièmement, parce que le domaine du ML évolue rapidement, il est préférable de construire une application de ML sous forme d’un ensemble de composants modulaires afin de pouvoir mettre à jour certains éléments sans perturber l’ensemble du *pipeline*. Comme discuté dans la [Section 2.2](/src/principles/index.qmd#sec-cloud-native), les technologies *cloud* permettent de créer des environnements de développement modulaires et évolutifs.

Cependant, comme également abordé dans la [Section 3](/src/implementation/index.qmd#sec-implementation), l’accès à ces ressources ne suffit pas. Un projet de ML nécessite une grande variété d’outils pour se conformer aux principes du MLOps : stockage des données, environnements de développement interactifs pour expérimenter librement, outils d’automatisation, outils de supervision, etc. Bien que ces outils puissent être installés sur un cluster Kubernetes, il est essentiel de les rendre disponibles aux *data scientists* de manière préconfigurée pour faciliter leur adoption. Grâce à son catalogue de services et à l’injection automatique de configurations dans les services, Onyxia permet de construire des projets qui reposent sur plusieurs composants *cloud* capables de communiquer facilement entre eux.

La manière dont l'entraînement du modèle a été réalisé pour ce projet illustre bien la flexibilité offerte par Onyxia pendant la phase d'expérimentation. Tout le code utilisé pour l'entraînement est écrit en Python au sein d’un service VSCode. Grâce à l’injection automatique des jetons d'accès S3 dans chaque service au démarrage, les différents utilisateurs du projet peuvent interagir directement avec les données d’entraînement stockées dans un *bucket* S3 sur MinIO. Toutes les expériences menées lors de la phase de sélection du modèle sont consignées dans une instance partagée de MLflow, qui enregistre les données sur une instance PostgreSQL automatiquement lancée sur Kubernetes, tandis que les artefacts (modèles entraînés et métadonnées associées) sont stockés sur MinIO.

Le modèle a été entraîné en utilisant une recherche exhaustive (*grid-search*) pour l’ajustement des hyperparamètres et évalué par validation croisée (*cross-validation*). Cette combinaison, reconnue pour offrir une meilleure évaluation des performances de généralisation du modèle, nécessite cependant d'importantes ressources de calcul en raison de la nature combinatoire du test de nombreuses combinaisons d’hyperparamètres. Sur le plan technique, ce processus de recherche a été implémenté à l'aide d’Argo Workflows, un moteur de *workflows* open source conçu pour orchestrer des tâches parallèles sur Kubernetes, chaque tâche étant spécifiée comme un conteneur indépendant. Cela a permis de comparer facilement les performances des différents modèles entraînés et de sélectionner le meilleur en utilisant les outils de comparaison et de visualisation disponibles dans l'interface utilisateur de MLflow.

En résumé, la phase d’entraînement a été rendue à la fois efficace et reproductible grâce à l’utilisation de nombreux composants modulaires interconnectés — une caractéristique distinctive des technologies *cloud* — mis à disposition des *data scientists* grâce à Onyxia.


### Déploiement d’un modèle

Une fois que les modèles candidats ont été optimisés, évalués et qu’un modèle performant a été sélectionné, l’étape suivante consiste à le rendre accessible aux utilisateurs finaux de l’application. Fournir simplement le modèle entraîné sous forme d’artefact, ou même uniquement le code pour l’entraîner, n’est pas une manière optimale de le transmettre car cela suppose que les utilisateurs disposent des ressources, de l’infrastructure et des connaissances nécessaires pour l’entraîner dans les mêmes conditions. L’objectif est donc de rendre le modèle accessible de manière simple et interopérable, c’est-à-dire qu’il doit être possible de l’interroger avec divers langages de programmation et par d'autres applications de manière programmatique.

Dans ce contexte, la solution retenue a été de déployer le modèle via une API REST. Cette technologie est devenue une norme pour servir des modèles de ML, car elle présente plusieurs avantages. Tout d’abord, elle s’intègre parfaitement dans un environnement orienté *cloud* : comme les autres composants de l'architecture choisie, elle permet d’interroger le modèle en utilisant des requêtes HTTP standard, ce qui contribue à la modularité du système. De plus, elle est interopérable : reposant sur des technologies standards pour les requêtes (requêtes HTTP) et les réponses (généralement une chaîne formatée en JSON), elle est largement indépendante du langage de programmation utilisé pour effectuer les requêtes. Enfin, les API REST offrent une grande évolutivité grâce à leur conception sans état (*stateless*)[^stateless-design]. Chaque requête contient toutes les informations nécessaires pour être comprise et traitée, ce qui permet de dupliquer facilement l’API sur différentes machines pour répartir une charge importante, dans une logique de scalabilité horizontale.

[^stateless-design]: La conception sans état (*stateless*) fait référence à une architecture système où chaque requête d’un client au serveur contient toutes les informations nécessaires pour comprendre et traiter la requête. Cela signifie que le serveur ne stocke aucune information sur l'état du client entre les requêtes, ce qui permet de traiter chaque requête indépendamment. Cette conception simplifie l'évolutivité et renforce la robustesse du système, car n'importe quel serveur peut gérer une requête sans dépendre des interactions précédentes.

L’API servant le modèle a été déployée avec FastAPI[^fastapi], un framework web accessible et bien documenté pour construire des APIs avec Python. Le code de l’API et les dépendances logicielles nécessaires sont encapsulés dans une image Docker, ce qui permet de la déployer sous forme de conteneur sur le cluster Kubernetes. L’un des avantages majeurs de Kubernetes est sa capacité d'adapter la puissance de l’API — via le nombre de pods d’API effectivement déployés — en fonction de la demande, tout en fournissant un équilibrage de charge automatique. Au démarrage, l’API récupère automatiquement le modèle approprié depuis l'entrepôt de modèles MLflow stocké sur MinIO. Enfin, comme le code de l’application est *packagé* en utilisant l’API standardisée de MLflow — permettant par exemple d’intégrer directement l’étape de prétraitement dans chaque appel API — le code d’inférence reste largement uniforme, quel que soit le framework de ML sous-jacent utilisé. Ce processus de déploiement est résumé dans la @fig-api-datalab.

[^fastapi]: [https://fastapi.tiangolo.com]()

![Une approche basé sur des technologies *cloud* pour servir un modèle de ML via une API REST](/figures/api-datalab.png){#fig-api-datalab width="75%"}

### Construction d'un *pipeline* intégré

L'architecture construite à ce stade reflète déjà certains principes importants du MLOps. L’utilisation de la conteneurisation pour déployer l’API, ainsi que celle de MLflow pour suivre les expérimentations pendant le développement du modèle, garantissent la reproductibilité des prédictions. L’utilisation de l'entrepôt central de modèles fourni par MLflow facilite la gestion du cycle de vie des modèles de manière collaborative. De plus, la modularité de l'architecture laisse de la place pour des améliorations ultérieures, puisque des composants modulaires peuvent être ajoutés ou modifiés facilement sans casser la structure du projet dans son ensemble. Comme nous le verrons dans les sections suivantes, cette propriété s’est avérée essentielle pour construire le projet de manière itérative, permettant d’ajouter une couche de supervision du modèle (@sec-monitoring) et un composant d’annotation (@sec-annotation) afin de favoriser l’amélioration continue du modèle en intégrant "l'humain dans le cycle de vie du modèle de ML" (*human in the loop*).

Cependant, la capacité à affiner l'architecture de base de manière itérative nécessite également une plus grande continuité dans le processus. À ce stade, le processus de déploiement implique plusieurs opérations manuelles. Par exemple, l’ajout d’une nouvelle fonctionnalité à l’API nécessiterait de construire une nouvelle image, de la taguer, de mettre à jour les manifestes Kubernetes utilisés pour déployer l’API et de les appliquer sur le cluster afin de remplacer l’instance existante avec un temps d’arrêt minimal. De même, un changement de modèle servi via l’API nécessiterait une simple modification du code, mais plusieurs étapes manuelles pour mettre à jour la version sur le cluster. En conséquence, les *data scientists* ne sont pas encore totalement autonomes dans cette configuration pour prototyper et tester des versions mises à jour du modèle ou de l’API, ce qui limite le potentiel d’amélioration continue.

Afin d'automatiser ce processus, un *pipeline* CI/CD — un concept déjà présenté dans @sec-devops-mlops — a été construit afin d'intégrer ces différentes étapes. La @fig-ci-cd illustre l'implémentation spécifique du *pipeline* CI/CD retenue pour le projet. Toute modification du code sur le dépôt de l’API, associée à un nouveau *tag*, déclenche un processus de CI (implémenté avec GitHub Actions) qui construit l’image Docker et la publie sur un registre public de conteneurs (DockerHub). Cette image peut ensuite être récupérée et déployée par l’orchestrateur de conteneurs (Kubernetes) en spécifiant et en appliquant manuellement de nouveaux manifestes pour mettre à jour les ressources Kubernetes de l’API. 

Cependant, cette approche présente un inconvénient : elle limite la reproductibilité du déploiement, car chaque ressource est gérée indépendamment par l’orchestrateur, et le cycle de vie du déploiement de l’API dans son ensemble n’est pas contrôlé. Pour pallier cette lacune, la partie déploiement a été intégrée dans un *pipeline* CD basée sur l’approche GitOps : les manifestes des ressources de l’API sont également stockés dans un dépôt Git. L’état de ce dépôt "GitOps" est surveillé par un opérateur Kubernetes (ArgoCD), de sorte à ce que toute modification des manifestes de l’application soit directement propagée au déploiement sur le cluster. Dans ce nouveau *pipeline* intégré de bout en bout, la seule action nécessaire pour déclencher une mise à jour de l’API est de modifier le tag de l’image de l’API indiquant la version à déployer.

![Le *pipeline* CI/CD implémenté dans le projet](/figures/ci-cd.png){#fig-ci-cd}

### Supervision d’un modèle en production {#sec-monitoring}

Une fois la phase initiale de développement du projet terminée — incluant l’entraînement, l’optimisation et le déploiement du modèle pour les utilisateurs —, il est crucial de comprendre que les responsabilités du *data scientist* ne s’arrêtent pas là. Traditionnellement, le rôle des *data scientists* se limite souvent à l’entraînement et à la sélection du modèle à déployer, le déploiement étant généralement délégué au département informatique. Cependant, une spécificité des projets de ML est que, une fois en production, le modèle n’a pas encore atteint la fin de son cycle de vie : il doit être surveillé en permanence afin d'éviter toute dégradation indésirable des performances. La supervision continue du modèle déployé est essentielle pour garantir la conformité des résultats aux spécifications, anticiper les changements dans les données et améliorer le modèle de manière itérative. Même en production, la compréhension fine de la problématique métier que cherche à résoudre l'approche ML reste entièrement nécessaire.

Le concept de supervision peut avoir différentes significations selon le contexte. Pour les équipes informatiques, il s’agit principalement de vérifier l’efficacité technique de l’application, notamment en termes de latence, de consommation de mémoire ou d’utilisation du disque de stockage. En revanche, pour les *data scientists* ou les équipes métier, la supervision est davantage centrée sur le suivi méthodologique du modèle. Cependant, le suivi en temps réel des performances d’un modèle de ML est souvent une tâche complexe. Contrairement à la phase d'entraînement, lors de laquelle on dispose de données labellisées qui permettent d'étalloner le modèle, en production cette "vérité terrain" (*ground truth*) n’est généralement pas connue au moment de la prédiction. Il est donc courant d’utiliser des proxys pour détecter les signes éventuels de dégradation des performances. Deux types principaux de dégradation d’un modèle ML sont généralement distingués. Le premier est le *data drift*, qui se produit lorsque la distribution des données utilisées pour l’inférence diffèrent significativement de celle des données utilisées lors de l’entraînement. Le second est le *concept drift*, qui survient lorsqu’un changement dans la relation statistique entre les variables explicatives et la variable cible est observé au fil du temps. Par exemple, le mot "Uber" était habituellement associé à des codes liés aux services de taxis. Cependant, avec l'apparition des services de livraison de repas comme "Uber Eats", cette relation entre le libellé et le code associé a changé. Il est donc nécessaire de repérer au plus tôt ces changements afin de ne pas dégrader la codification.

Dans le cadre du projet, l’objectif est d’atteindre le taux le plus élevé possible de libellés correctement classifiés, tout en minimisant le nombre de descriptions nécessitant une intervention manuelle. Ainsi, le but est de distinguer les prédictions correctes des prédictions incorrectes sans avoir accès au préalable à la vérité terrain. Pour y parvenir, on calcule un indice de confiance, défini comme la différence entre les deux scores de confiance les plus élevés parmi les résultats renvoyés par le modèle. Pour une description textuelle donnée, si l’indice de confiance dépasse un seuil déterminé, la description est automatiquement codée. Sinon, elle est codée manuellement par un agent de l’Insee. Cette tâche de codification manuelle est néanmoins assistée par le modèle ML : via une application qui interroge l’API, l’agent visualise les cinq codes les plus probables selon le modèle. Le seuil choisi pour l'indice de confiance est un paramètre que l'équipe métier peut utiliser pour arbitrer entre la charge de travail qu'elle est disposée à assumer pour la reprise manuelle et le taux d'erreurs qu'elle est prête à tolérer.

### Définition du seuil de codification automatique et supervision en production

La définition du seuil pour la codification automatique des descriptions textuelles a été une étape cruciale de ce processus, nécessitant un compromis entre un taux élevé de codification automatique et une erreur minimale de classification. Afin de surveiller le comportement du modèle en production, un tableau de bord interactif a été développée pour permettre de visualiser plusieurs métriques d’intérêt pour les équipes métier. Parmi ces métriques figurent le nombre de requêtes par jour et le taux de codification automatique quotidien, pour un seuil donné de l’indice de confiance. Cette visualisation permet aux équipes métier de connaître le taux de codification automatique qu’elles auraient obtenu si elles avaient choisi différents seuils. Le tableau de bord représente également la distribution des indices de confiance obtenus et compare des fenêtres temporelles afin de détecter des changements dans les distributions des prédictions renvoyées par le modèle[^distance-distrib]. Enfin, les indices de confiance peuvent être analysés à des niveaux de granularité plus fins, basés sur les niveaux d’agrégation de la classification statistique, pour identifier les classes les plus difficiles à prédire et celles qui sont plus ou moins fréquentes.

[^distance-distrib]: Ces changements de distribution sont généralement vérifiés en calculant des distances statistiques — telles que la distance de Bhattacharyya, la divergence de Kullback-Leibler ou la distance de Hellinger — et/ou en effectuant des tests statistiques — tels que le test de Kolmogorov–Smirnov ou le test du khi-deux.

La @fig-full-architecture détaille les composants ajoutés à l’architecture du projet pour fournir le tableau de bord de supervision décrit ci-dessus. Tout d’abord, un processus Python (deuxième composant de la rangée inférieure) a été développé pour récupérer quotidiennement les logs de l’API et les transforme en fichiers partitionnés au format Parquet[^custom-framework]. Ensuite, [^quarto] pour construire un tableau de bord interactif (troisième composant de la rangée inférieure). Pour calculer les diverses métriques présentées dans le tableau de bord, les fichiers Parquet sont interrogés via le moteur optimisé DuckDB. À l’instar de l’API, le tableau de bord est construit et déployé sous forme de conteneur sur le cluster Kubernetes, et ce processus est également automatisé grâce à un *pipeline* CI/CD. Le composant d’annotation (quatrième composant de la rangée inférieure) est discuté dans la section suivante.

[^custom-framework]: Idéalement, les *frameworks* existants devraient être privilégiés par rapport aux solutions sur mesure pour adopter des routines standardisées. Lors de la construction de ce composant du *pipeline*, on a pu constater que les frameworks *cloud* existants pour l’analyse des logs présentaient d’importantes limites. Cela constitue une piste d’amélioration pour le projet.

[^quarto]: Successeur de R Markdown, Quarto est devenu un outil essentiel. Il unifie les fonctionnalités de plusieurs packages très utiles de l’écosystème R Markdown tout en offrant une prise en charge native de plusieurs langages de programmation, dont Python et Julia en plus de R. Il est de plus en plus utilisé à l’Insee pour produire des documents reproductibles et les exporter dans divers  (HTML, PDF, ODT, etc.).

![Implémentation d’une architecture MLOps retenur pour le projet](/figures/annotation-datalab.png){#fig-full-architecture}

### Favoriser l'amélioration continue du modèle {#sec-annotation}

La composante de supervision du modèle fournit une vue détaillée et essentielle de l'activité du modèle en production. En raison de la nature dynamique des données de production, les performances des modèles de ML ont tendance à diminuer avec le temps. Pour favoriser l’amélioration continue du modèle, il est donc essentiel de mettre en place des stratégies permettant de surmonter ces pertes de performance. Une stratégie couramment utilisée est le réentraînement périodique du modèle, nécessitant la collecte de nouvelles données d’entraînement plus récentes et donc plus proches de celle observées en production.

Plusieurs mois après le déploiement de la première version du modèle en production, le besoin de mettre en œuvre un processus d’annotation continue est devenu de plus en plus évident pour deux raisons principales. Premièrement, un échantillon de référence (*gold standard*) n’était pas disponible lors de la phase d’expérimentation. Un sous-ensemble des données d’entraînement a donc été utilisé pour l’évaluation, tout en sachant que la qualité de la labelisation n’était pas optimale. La collecte continue d’un échantillon de référence permettrait ainsi d’obtenir une vue réaliste des performances du modèle en production sur des données réelles, en particulier sur les données codifiées automatiquement. Deuxièmement, la refonte de la nomenclature statistique APE prévue en 2025 impose aux INS d’adopter la dernière version. Cette révision, qui introduit des changements importants, nécessite une adaptation du modèle et surtout la création d’un nouveau jeu de données d’entraînement. L’annotation de l’ancien jeu de données d’entraînement selon la nouvelle nomenclature statistique est donc indispensable.

Dans ce contexte, une campagne d’annotation a été lancée début 2024 pour construire de manière continue un jeu de données de référence. Cette campagne est réalisée sur le SSP Cloud en utilisant le service Label Studio, un outil *open source* d’annotation offrant une interface ergonomique et disponible dans le catalogue d’Onyxia. La @fig-full-architecture montre comment le composant d’annotation (quatrième composant de la rangée inférieure) a pu être intégré facilement dans l’architecture du projet grâce à sa nature modulaire. En pratique, un échantillon de descriptions textuelles est tiré aléatoirement des données passées par l’API au cours des trois derniers mois. Cet échantillon est ensuite soumis à l’annotation par des experts APE via l’interface de Label Studio. Les résultats de l’annotation sont automatiquement sauvegardés sur MinIO, transformés au format Parquet, puis intégrés directement dans le tableau de bord de supervision pour calculer et observer diverses métriques de performance du modèle. Ces métriques offrent une vision beaucoup plus précise des performances réelles du modèle sur les données de production, et permet notamment de détecter les cas les plus problématiques.

En parallèle, une campagne d’annotation pour construire un nouveau jeu d’entraînement adapté à la NAF 2025 a également été réalisée. En exploitant à la fois les nouvelles données d’entraînement et les métriques de performance dérivées de l’échantillon de référence, on peut espérer améliorer la précision du modèle de manière itérative grâce à des réentraînements périodiques et automatique dès lors que le moteur de codification aura migré sur le cluster Kubernetes de production.
